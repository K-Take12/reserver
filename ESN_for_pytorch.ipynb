{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yZSWjcj3RXoK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 恒等写像\n",
        "def identity(x):\n",
        "    return x # pytorchのテンソルをそのまま処理可能\n",
        "\n",
        "# 入力層\n",
        "class Input(nn.Module):\n",
        "    def __init__(self, N_u, N_x, input_scale, seed=0):\n",
        "        '''\n",
        "        param N_u: 入力次元\n",
        "        param N_x: リザバーのノード数\n",
        "        param input_scale: 入力スケーリング\n",
        "        '''\n",
        "        super(Input, self).__init__()\n",
        "        # 一様分布に従う乱数\n",
        "        torch.manual_seed(seed)\n",
        "        self.Win = torch.empty((N_x, N_u)).uniform_(-input_scale, input_scale)\n",
        "        self.Win.requires_grad = False\n",
        "\n",
        "        return self.Win\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class Reservoir(nn.Module):\n",
        "    def __init__(self, N_x, density, rho, activation_func, leaking_rate, seed):\n",
        "        '''\n",
        "        param N_x: リザバーのノード数\n",
        "        param density: ネットワークの結合密度\n",
        "        param rho: リカレント結合重み行列のスペクトル半径\n",
        "        param activation_func: ノードの活性化関数 (torch.nn.functional)\n",
        "        param leaking_rate: leaky integratorモデルのリーク率\n",
        "        param seed: 乱数の種\n",
        "        '''\n",
        "        super(Reservoir, self).__init__()\n",
        "        self.seed = seed\n",
        "        self.N_x = N_x\n",
        "        self.W = self.make_connection_pytorch(N_x, density, rho, seed)\n",
        "        self.W = nn.Parameter(self.W)  # Wをnn.Parameterとしてラップ\n",
        "        self.x = torch.zeros(N_x, dtype=torch.float32)  # リザバー状態ベクトルの初期化\n",
        "        self.activation_func = activation_func\n",
        "        self.alpha = leaking_rate\n",
        "        self.requires_grad = False  # リザバーの重みは訓練しない\n",
        "\n",
        "    @staticmethod\n",
        "    def make_connection_pytorch(N_x, density, rho, seed):\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        # ランダムな接続行列の生成\n",
        "        connection = torch.bernoulli(torch.full((N_x, N_x), density))\n",
        "        rec_scale = 1.0\n",
        "        W = connection * (torch.rand((N_x, N_x)) * 2 * rec_scale - rec_scale)\n",
        "\n",
        "        # Wが空でないことを確認\n",
        "        if W.numel() == 0:\n",
        "            raise ValueError(\"Weight matrix W is empty. Please check the matrix generation process.\")\n",
        "\n",
        "        # スペクトル半径を計算\n",
        "        eigenvalues = torch.linalg.eigvals(W)\n",
        "        if eigenvalues.numel() == 0:\n",
        "            raise ValueError(\"Eigenvalues are empty. Check the matrix W.\")\n",
        "\n",
        "        sp_radius = torch.max(torch.abs(eigenvalues.real))\n",
        "\n",
        "        # スペクトル半径をrhoにスケーリング\n",
        "        W *= rho / sp_radius\n",
        "\n",
        "        return W\n",
        "\n",
        "    def forward(self, x_in):\n",
        "      '''\n",
        "      param x: N_x次元のベクトル (torch.Tensor)\n",
        "      return: N_x次元のベクトル (torch.Tensor)\n",
        "      '''\n",
        "      # グラフを切り離す\n",
        "      self.x = (1.0 - self.alpha) * self.x.detach() + self.alpha * self.activation_func(\n",
        "          torch.matmul(self.W, self.x) + x_in\n",
        "      )\n",
        "      return self.x\n"
      ],
      "metadata": {
        "id": "gEDbclHbSB9h"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class Output(nn.Module):\n",
        "    def __init__(self, N_x, N_y, seed=0):\n",
        "        '''\n",
        "        param N_x: リザバーのノード数\n",
        "        param N_y: 出力次元\n",
        "        param seed: 乱数の種\n",
        "        '''\n",
        "        super(Output, self).__init__()\n",
        "        # 正規分布に従う乱数で初期化\n",
        "        torch.manual_seed(seed)\n",
        "        self.Wout = torch.randn(N_y, N_x, dtype=torch.float32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        param x: N_x次元のベクトル (torch.Tensor)\n",
        "        return: N_y次元のベクトル (torch.Tensor)\n",
        "        '''\n",
        "        return torch.matmul(self.Wout, x)\n",
        "\n",
        "    def setweight(self, Wout_opt):\n",
        "        '''\n",
        "        学習済みの出力結合重み行列を設定\n",
        "        param Wout_opt: 新しい重み行列 (torch.Tensor)\n",
        "        '''\n",
        "        if not isinstance(Wout_opt, torch.Tensor):\n",
        "            raise ValueError(\"Wout_opt must be a torch.Tensor.\")\n",
        "        self.Wout = Wout_opt\n"
      ],
      "metadata": {
        "id": "I5_bmE4KYjWR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class Feedback:\n",
        "    def __init__(self, N_y, N_x, fb_scale, seed=0):\n",
        "        '''\n",
        "        param N_y: 出力次元\n",
        "        param N_x: リザバーのノード数\n",
        "        param fb_scale: フィードバックスケーリング\n",
        "        param seed: 乱数の種\n",
        "        '''\n",
        "        # 一様分布に従う乱数で初期化\n",
        "        torch.manual_seed(seed)\n",
        "        self.Wfb = (torch.rand(N_x, N_y, dtype=torch.float32) * 2 * fb_scale - fb_scale)\n",
        "\n",
        "    def __call__(self, y):\n",
        "        '''\n",
        "        param y: N_y次元のベクトル (torch.Tensor)\n",
        "        return: N_x次元のベクトル (torch.Tensor)\n",
        "        '''\n",
        "        return torch.matmul(self.Wfb, y)\n"
      ],
      "metadata": {
        "id": "WIsZCFQrd6od"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ESN(nn.Module):\n",
        "    def __init__(self, N_u, N_y, N_x, density=0.05, input_scale=1.0, rho=0.95, activation_func=torch.tanh, fb_scale=None, fb_seed=0, noise_level=None, leaking_rate=0.9, output_func=None, classification=False, average_window=None, seed = 40):\n",
        "        super(ESN, self).__init__()\n",
        "\n",
        "        # モジュールの初期化\n",
        "        self.Input = Input(N_u, N_x, input_scale)\n",
        "        self.Reservoir = Reservoir(N_x, density, rho, activation_func, leaking_rate, seed)\n",
        "        self.Output = Output(N_x, N_y)\n",
        "        self.N_u = N_u\n",
        "        self.N_y = N_y\n",
        "        self.N_x = N_x\n",
        "        self.output_func = output_func if output_func is not None else lambda x: x\n",
        "        self.classification = classification\n",
        "        self.alpha = leaking_rate\n",
        "        self.Input.requires_grad = False\n",
        "        self.Reservoir.requires_grad = False\n",
        "\n",
        "        # 出力層からのリザバーへのフィードバックの有無\n",
        "        if fb_scale is None:\n",
        "            self.Feedback = None\n",
        "        else:\n",
        "            self.Feedback = Feedback(N_y, N_x, fb_scale, fb_seed)\n",
        "\n",
        "        # リザバーの状態更新にノイズを加えるか\n",
        "        if noise_level is None:\n",
        "            self.noise = None\n",
        "        else:\n",
        "            self.noise = torch.rand(N_x, 1) * noise_level * 2 - noise_level  # -noise_level to +noise_level\n",
        "\n",
        "        # 分類問題の場合の設定\n",
        "        if classification:\n",
        "            if average_window is None:\n",
        "                raise ValueError('Window for time average is not given!')\n",
        "            else:\n",
        "                self.window = torch.zeros((average_window, self.N_x))\n",
        "\n",
        "        # リザバー状態の初期化\n",
        "        self.reset_reservoir_state()\n",
        "\n",
        "    def forward(self, u):\n",
        "        '''\n",
        "        param u: N_u次元のベクトル (torch.Tensor)\n",
        "        return: N_y次元のベクトル (torch.Tensor)\n",
        "        '''\n",
        "        train_len = len(u)\n",
        "        Y = []\n",
        "\n",
        "        for n in range(train_len):\n",
        "          x_in = self.Input(u[n])\n",
        "        # リザバー状態ベクトル\n",
        "          x = self.Reservoir(x_in)\n",
        "\n",
        "          y = self.Output(x)\n",
        "          Y.append(self.output_func(y))\n",
        "\n",
        "        # フィードバックがある場合の処理\n",
        "        if self.Feedback is not None:\n",
        "            self.x += self.Feedback(self.x)\n",
        "\n",
        "        # ノイズの追加\n",
        "        if self.noise is not None:\n",
        "            self.x += self.noise\n",
        "\n",
        "        # 学習前のモデル出力\n",
        "\n",
        "        return torch.stack(Y)\n",
        "\n",
        "    def reset_reservoir_state(self):\n",
        "        '''リザバー状態ベクトルの初期化'''\n",
        "        self.x = torch.zeros(self.N_x)"
      ],
      "metadata": {
        "id": "3BNvAA6bd8Px"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # test\n",
        "# 複雑な波形データを生成（複数のサイン波を合成）\n",
        "# 複雑な波形データを生成（複数のサイン波を合成）\n",
        "def generate_complex_wave(seq_length=100, n_features=1, freqs=[0.1, 0.3, 0.5], amplitudes=[1, 0.5, 0.2]):\n",
        "    t = np.linspace(0, seq_length * np.pi, seq_length)\n",
        "    data = np.zeros_like(t)\n",
        "\n",
        "    # 複数の周波数成分のサイン波を合成\n",
        "    for f, a in zip(freqs, amplitudes):\n",
        "        data += a * np.sin(t * f)\n",
        "\n",
        "    return data.reshape(-1, n_features)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 訓練データの準備\n",
        "# seq_length = 50\n",
        "# n_features = 1  # 単一の特徴量（単一のチャネル）\n",
        "# freqs = [0.1, 0.3, 0.5]\n",
        "# amplitudes = [1, 0.5, 0.2]\n",
        "# train_data = torch.tensor(generate_complex_wave(500, n_features=1), dtype=torch.float32)\n",
        "# train_data = generate_complex_wave(seq_length=seq_length, n_features=n_features, freqs=freqs, amplitudes=amplitudes)\n",
        "# train_data = torch.tensor(train_data, dtype=torch.float32)\n",
        "# target_data = torch.tensor(generate_complex_wave(seq_length=seq_length, n_features=n_features, freqs=freqs, amplitudes=amplitudes), dtype=torch.float32)\n",
        "# target_data = torch.tensor(target_data, dtype=torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "# 訓練データの準備（修正なしの部分）\n",
        "seq_length = 50\n",
        "n_features = 1\n",
        "freqs = [0.1, 0.3, 0.5]\n",
        "amplitudes = [1, 0.5, 0.2]\n",
        "\n",
        "train_data = torch.tensor(generate_complex_wave(seq_length=seq_length, n_features=n_features, freqs=freqs, amplitudes=amplitudes), dtype=torch.float32)\n",
        "target_data = train_data.clone().detach()  # 修正: tensorのコピーを使用\n",
        "\n",
        "# モデル初期化\n",
        "N_u, N_y, N_x = 1, 1, 200\n",
        "esn_model = ESN(N_u, N_y, N_x)\n",
        "\n",
        "# 最適化設定\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(esn_model.parameters(), lr=1e-3)\n",
        "\n",
        "# 訓練ループ\n",
        "train_len = len(train_data)\n",
        "for epoch in range(10):  # エポック数を指定\n",
        "    esn_model.reset_reservoir_state()  # リザバー状態をリセット\n",
        "    optimizer.zero_grad()  # 勾配を初期化\n",
        "\n",
        "    # モデルの順伝播\n",
        "    predictions = esn_model(train_data)\n",
        "\n",
        "    # ロス計算\n",
        "    loss = criterion(predictions, target_data)\n",
        "\n",
        "    # 誤差逆伝播\n",
        "    loss.backward()  # retain_graph=Trueを外す\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "# 訓練後のモデルで予測\n",
        "model.reset_reservoir_state()\n",
        "predicted_output = model(train_data).detach().numpy()\n",
        "predicted_output = predicted_output.transpose(0,1)\n",
        "print(len(predicted_output))\n",
        "\n",
        "# 結果の描画\n",
        "plt.plot(train_data.numpy(), label='True Waveform')\n",
        "plt.plot(predicted_output, label='Predicted Waveform', linestyle='--')\n",
        "plt.legend()\n",
        "plt.title('True vs Predicted Waveform')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CwvBiJAetU0g",
        "outputId": "3dc86967-7d73-40b4-eeb6-31b9db417f8f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 748.73876953125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Error detected in MulBackward0. Traceback of forward call that caused the error:\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-13-7419c6ac7f0e>\", line 56, in <cell line: 51>\n",
            "    predictions = esn_model(train_data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"<ipython-input-12-d373cc832981>\", line 51, in forward\n",
            "    x = self.Reservoir(x_in)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"<ipython-input-11-e176e0bf85c8>\", line 56, in forward\n",
            "    self.x = (1.0 - self.alpha) * self.x.detach() + self.alpha * self.activation_func(\n",
            " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:110.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7419c6ac7f0e>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m    \u001b[0;31m# 誤差逆伝播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m    \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# retain_graph=Trueを外す\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7L4DlGQ-x23j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}